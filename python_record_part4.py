# -*- coding: utf-8 -*-
"""python_record_part4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c3Da7DC2j0S96sdxanmPxaPutjMd7Zh-

4.1.1 Create a numpy array filled with all ones by defining its shape.
"""

import numpy as np
shape=(3,3)
ones_array=np.ones(shape)
print(ones_array)

"""4.1.2)How do you remove rows from a Numpy array that contains non-numeric values?

"""

import numpy as np
arr=np.array([
    [1,2,3,4],
    ['a','b',4,7],
    [11,12,13,14]
],dtype=object)
def safe_convert(value):
    try:
        return float(value)
    except ValueError:
        return np.nan
arr_numeric =np.vectorize(safe_convert)(arr)
filtered_arr=arr_numeric[~np.isnan(arr_numeric).any(axis=1)]
print("original array:",arr)
print("Filtered Array:",filtered_arr)

"""4.1.3)Remove single-dimensional entries from the shape of an array


"""

import numpy as np

# Example array with single-dimensional entries
arr = np.array([[[1], [2], [3]]])
print("Original array shape:", arr.shape)

# Remove single-dimensional entries from the shape
squeezed_arr = np.squeeze(arr)
print("Squeezed array shape:", squeezed_arr.shape)
print("Squeezed array:\n", squeezed_arr)

"""4.1.4)How do you check whether specified values are present in the NumPy array?"""

import numpy as np

# Create a NumPy array
arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])

# Values to check
values_to_check = [3, 5, 10]

# Check for presence of specified values
presence = np.isin(values_to_check, arr)

print("Values to check:", values_to_check)
print("Presence in the array:", presence)

"""4.1.5)Write a program to get all 2D diagonals of a 3D NumPy array?"""

import numpy as np

# Create a 3D NumPy array
array_3d = np.array([
    [[1, 2, 3],
     [4, 5, 6],
     [7, 8, 9]],

    [[10, 11, 12],
     [13, 14, 15],
     [16, 17, 18]],

    [[19, 20, 21],
     [22, 23, 24],
     [25, 26, 27]]
])

# Function to get all 2D diagonals of a 3D NumPy array
def get_2d_diagonals(arr):
    diagonals = []
    # Iterate over the first dimension of the 3D array
    for i in range(arr.shape[0]):
        # Get the diagonal of each 2D slice
        diag = np.diagonal(arr[i], axis1=0, axis2=1)
        diagonals.append(diag)
    return diagonals

# Get all 2D diagonals
diagonals_2d = get_2d_diagonals(array_3d)

# Print the results
print("3D Array:\n", array_3d)
print("\n2D Diagonals:")
for index, diag in enumerate(diagonals_2d):
    print(f"Diagonal from slice {index}:\n{diag}")

"""4.1.6)Write a NumPy program to sort a given array of shape 2 along the first axis, last axis, and flattened array."""

import numpy as np

# Create a sample array of shape (2, 3)
array = np.array([[3, 1, 2],
                  [6, 4, 5]])

print("Original array:\n", array)

# Sort along the first axis (axis=0)
sorted_first_axis = np.sort(array, axis=0)
print("\nSorted along the first axis (axis=0):\n", sorted_first_axis)

# Sort along the last axis (axis=1)
sorted_last_axis = np.sort(array, axis=1)
print("\nSorted along the last axis (axis=1):\n", sorted_last_axis)

# Flatten the array and sort
flattened_sorted = np.sort(array.flatten())
print("\nSorted flattened array:\n", flattened_sorted)

"""4.1.7)Write a NumPy program to create a structured array from a given student name, height, class, and data type. Now sort by class, then height if the classes are equal."""

import numpy as np

# Define the data type for the structured array
dtype = [('name', 'U20'), ('height', 'f4'), ('class', 'i4')]

# Create a structured array with student data
student_data = np.array([
    ('Alice', 5.5, 2),
    ('Bob', 6.0, 1),
    ('Charlie', 5.7, 2),
    ('David', 5.9, 1),
    ('Eve', 5.4, 2)
], dtype=dtype)

print("Original structured array:\n", student_data)

# Sort the structured array by 'class' and then by 'height'
sorted_students = np.sort(student_data, order=['class', 'height'])

print("\nSorted structured array:\n", sorted_students)

"""4.1.8)Write a NumPy program to sort a given complex array using the real part first, then the imaginary part."""

import numpy as np

# Create a complex NumPy array
complex_array = np.array([3 + 2j, 1 + 4j, 2 + 3j, 1 + 2j, 2 + 1j])

print("Original complex array:\n", complex_array)

# Sort by real part, then by imaginary part
sorted_indices = np.lexsort((complex_array.imag, complex_array.real))
sorted_complex_array = complex_array[sorted_indices]

print("\nSorted complex array:\n", sorted_complex_array)

"""4.1.9)Write a NumPy program to sort a given array by the nth column."""

import numpy as np

# Create a sample 2D NumPy array
array = np.array([[3, 1, 2],
                  [6, 4, 5],
                  [1, 7, 8],
                  [9, 0, 3]])

print("Original array:\n", array)

# Specify the column index to sort by (0-indexed)
n = 1  # Sort by the second column

# Sort the array by the nth column
sorted_array = array[array[:, n].argsort()]

print(f"\nArray sorted by column {n}:\n", sorted_array)

"""4.1.10)Calculate the sum of the diagonal elements of a NumPy array"""

import numpy as np

# Create a sample 2D NumPy array
array = np.array([[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9]])

print("Original array:\n", array)

# Calculate the sum of the diagonal elements
diagonal_sum = np.sum(np.diagonal(array))

print("Sum of diagonal elements:", diagonal_sum)

"""4.1.11)Write a program for Matrix Multiplication in NumPy"""

import numpy as np

# Create two sample matrices
matrix_a = np.array([[1, 2, 3],
                      [4, 5, 6]])

matrix_b = np.array([[7, 8],
                      [9, 10],
                      [11, 12]])

print("Matrix A:\n", matrix_a)
print("\nMatrix B:\n", matrix_b)

# Method 1: Using numpy.dot()
result_dot = np.dot(matrix_a, matrix_b)
print("\nMatrix Multiplication using np.dot():\n", result_dot)

# Method 2: Using the @ operator
result_at = matrix_a @ matrix_b
print("\nMatrix Multiplication using @ operator:\n", result_at)

"""4.1.12)Multiply matrices of complex numbers using NumPy in Python

"""

import numpy as np

# Create two sample matrices with complex numbers
matrix_a = np.array([[1 + 2j, 2 + 3j],
                      [3 + 4j, 4 + 5j]])

matrix_b = np.array([[5 + 6j, 6 + 7j],
                      [7 + 8j, 8 + 9j]])

print("Matrix A:\n", matrix_a)
print("\nMatrix B:\n", matrix_b)

# Perform matrix multiplication
result = np.dot(matrix_a, matrix_b)

# Alternatively, you can use the @ operator
# result = matrix_a @ matrix_b

print("\nMatrix Multiplication Result:\n", result)

"""4.1.13)Calculate the inner, outer, and cross products of matrices and vectors using NumPy.

"""

import numpy as np

# Define two vectors
vector_a = np.array([1, 2, 3])
vector_b = np.array([4, 5, 6])

# Calculate the inner product
inner_product = np.inner(vector_a, vector_b)
print("Inner Product of vector_a and vector_b:", inner_product)

# Calculate the outer product
outer_product = np.outer(vector_a, vector_b)
print("\nOuter Product of vector_a and vector_b:\n", outer_product)

# Calculate the cross product
cross_product = np.cross(vector_a, vector_b)
print("\nCross Product of vector_a and vector_b:", cross_product)

# Define two matrices
matrix_a = np.array([[1, 2],
                     [3, 4]])

matrix_b = np.array([[5, 6],
                     [7, 8]])

# Calculate the inner product of matrices (dot product)
matrix_inner_product = np.dot(matrix_a, matrix_b)
print("\nInner Product (Dot Product) of matrix_a and matrix_b:\n", matrix_inner_product)

# Calculate the outer product of the first column of matrix_a and the first column of matrix_b
outer_product_matrix = np.outer(matrix_a[:, 0], matrix_b[:, 0])
print("\nOuter Product of the first columns of matrix_a and matrix_b:\n", outer_product_matrix)

"""4.1.14)Compute the covariance matrix of two given NumPy arrays.

"""

import numpy as np

# Define two sample NumPy arrays (samples of two variables)
data1 = np.array([1, 2, 3, 4, 5])
data2 = np.array([5, 4, 3, 2, 1])

# Stack the arrays vertically to create a 2D array
data = np.vstack((data1, data2))

# Compute the covariance matrix
covariance_matrix = np.cov(data)

print("Data 1:\n", data1)
print("Data 2:\n", data2)
print("\nCovariance Matrix:\n", covariance_matrix)

"""4.1.15)Convert covariance matrix to correlation matrix using Python

"""

import numpy as np

# Define a sample covariance matrix
covariance_matrix = np.array([[4, 2],
                               [2, 3]])

print("Covariance Matrix:\n", covariance_matrix)

# Calculate the standard deviations for each variable
std_devs = np.sqrt(np.diagonal(covariance_matrix))

# Create a correlation matrix
correlation_matrix = covariance_matrix / std_devs[:, None] / std_devs[None, :]

print("\nCorrelation Matrix:\n", correlation_matrix)

"""4.1.16)Write a NumPy program to compute the histogram of nums against the bins."""

import numpy as np
import matplotlib.pyplot as plt

# Define a sample array of numbers (nums)
nums = np.array([1, 2, 1, 3, 4, 5, 6, 5, 4, 3, 2, 1])

# Define the bins
bins = np.array([0, 1, 2, 3, 4, 5, 6, 7])

# Compute the histogram
histogram, bin_edges = np.histogram(nums, bins)

print("Histogram:", histogram)
print("Bin edges:", bin_edges)

# Plotting the histogram
plt.hist(nums, bins=bins, edgecolor='black', alpha=0.7)
plt.title("Histogram of nums")
plt.xlabel("Value")
plt.ylabel("Frequency")
plt.xticks(bin_edges)
plt.show()

"""4.1.17)Write a NumPy program to compute the cross-correlation of two given arrays"""

import numpy as np
import matplotlib.pyplot as plt

# Define two sample arrays
array_x = np.array([1, 2, 3, 4, 5])
array_y = np.array([5, 4, 3, 2, 1])

# Compute the cross-correlation
cross_correlation = np.correlate(array_x, array_y, mode='full')

print("Array X:", array_x)
print("Array Y:", array_y)
print("\nCross-Correlation:", cross_correlation)

# Plotting the cross-correlation
plt.stem(cross_correlation)
plt.title("Cross-Correlation")
plt.xlabel("Lag")
plt.ylabel("Cross-Correlation Value")
plt.grid()
plt.show()

"""4.1.18)Write a NumPy program to compute the mean, standard deviation, and variance of a given array along the second axis."""

import numpy as np

# Define a sample 2D array
array = np.array([[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9]])

print("Original Array:\n", array)

# Compute the mean along the second axis (axis 1)
mean = np.mean(array, axis=1)
print("\nMean along the second axis:", mean)

# Compute the standard deviation along the second axis (axis 1)
std_deviation = np.std(array, axis=1)
print("Standard Deviation along the second axis:", std_deviation)

# Compute the variance along the second axis (axis 1)
variance = np.var(array, axis=1)
print("Variance along the second axis:", variance)

"""4.1.19)Write a NumPy program to compute the 80th percentile for all elements in a given array along the second axis."""

import numpy as np

# Define a sample 2D array
array = np.array([[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9]])

print("Original Array:\n", array)

# Compute the 80th percentile along the second axis (axis 1)
percentile_80 = np.percentile(array, 80, axis=1)

print("\n80th Percentile along the second axis:", percentile_80)

"""4.2.1)	Write a Pandas program to add, subtract, multiply, and divide two Pandas Series."""

import pandas as pd
series1 = pd.Series([10, 20, 30, 40, 50])
series2 = pd.Series([5, 10, 15, 20, 25])
addition = series1 + series2
print("Addition of two Series:")
print(addition)
subtraction = series1 - series2
print("\nSubtraction of two Series:")
print(subtraction)
multiplication = series1 * series2
print("\nMultiplication of two Series:")
print(multiplication)
division = series1 / series2
print("\nDivision of two Series:")
print(division)

"""4.2.2.	Write a Pandas program to convert a dictionary to a Pandas series."""

import pandas as pd
my_dict={'a':10,'b':12,'c':30}
print("Dictionary:",my_dict)
my_series=pd.Series(my_dict)
print("Series:",my_series)

"""4.2.3.	Write a Pandas program to convert the first column of a data frame  into a Series"""

import pandas as pd

# Sample DataFrame
my_data = {'Column1': [10, 20, 30, 40], 'Column2': [100, 200, 300, 400]}
df = pd.DataFrame(my_data)

# Convert the first column into a Series
series = df.iloc[:, 0]

# Display the Series
print(series)

"""4.2.4.	Write a Pandas program to convert a Series of lists into one Series."""

import pandas as pd

# Sample Series of lists
series_of_lists = pd.Series([[1, 2, 3], [4, 5], [6, 7, 8]])

# Convert Series of lists into one Series
flattened_series = series_of_lists.explode().reset_index(drop=True)

# Display the result
print(flattened_series)

"""4.2.5.	Write a Pandas program to create a subset of a given series based on value and condition"""

import pandas as pd
data=pd.Series([1,2,3,4,5,6,7,8])
subset=data[data>5]
print(subset)

"""4.2.6.	Write a Pandas program to get the items that are not common in two given series."""

import pandas as pd

# Sample Series
series1 = pd.Series([1, 2, 3, 4, 5])
series2 = pd.Series([4, 5, 6, 7, 8])

# Get items that are not common in both series
uncommon_items = pd.concat([series1[~series1.isin(series2)], series2[~series2.isin(series1)]])

# Display the result
print(uncommon_items)

"""4.2.7.	Write a Pandas program to calculate the frequency counts of each unique value of a given series"""

import pandas as pd

# Sample Series
data = pd.Series([1, 2, 2, 3, 4, 4, 4, 5, 5])

# Calculate frequency counts of each unique value
frequency_counts = data.value_counts()

# Display the result
print(frequency_counts)

"""4.2.8.	Write a Pandas program to filter words from a given series that contain at least two vowelsu."""

import pandas as pd

# Sample Series
data = pd.Series(['apple', 'banana', 'grape', 'kiwi', 'pear', 'mango'])

# Function to count vowels in a word
def count_vowels(word):
    vowels = set('aeiouAEIOU')
    return sum(1 for char in word if char in vowels)

# Filter words with at least two vowels
filtered_words = data[data.apply(lambda word: count_vowels(word) >= 2)]

# Display the result
print(filtered_words)

"""4.2.9.	Write a Pandas program to find the index of the first occurrence of the smallest and largest values of a given series."""

import pandas as pd
data=pd.Series([2,67,100,0,3,7,8])
min_index=data.idxmin()
max_index=data.idxmax()
print("index of minimum value",min_index)
print("index of max value",max_index)

"""4.2.11.	Write a Pandas program to select the 'name' and 'score' columns from a data frame."""

import pandas as pd
data={'name':['varghese','kuku','aami'],
      'rollno':[12,45,19],
      'age':[21,22,23]}
df=pd.DataFrame(data)
selected_columns=df[['name','age']]
print(selected_columns)

"""4.2.12.	Write a Pandas program to count the number of rows and columns in a data frame."""

import pandas as pd
data={'name':['varghese','kuku','aami'],
      'rollno':[34,78,12],
      'age':[14,34,23]}
df=pd.DataFrame(data)
num_rows=df.shape[0]
num_cols=df.shape[1]
print("no of rows:",num_rows)
print("no of columns",num_cols)

"""4.2.13.	Write a Pandas program to add one row to an existing data frame"""

import pandas as pd
data={'name':['varghese','kuku','aami'],
      'rollno':[23,12,45],
      'age':[22,21,20]}
df=pd.DataFrame(data)
print("original data frame",df)
new_row={'name':'roy','rollno':62,'age':24}
df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
print("DataFtame after adding new row:",df)

"""4.2.14.	Write a Pandas program to write a data frame to a CSV file using a tab separator."""

import pandas as pd

# Creating a sample DataFrame
data = {'name': ['varghese', 'kuku', 'aami'],
        'rollno': [23, 12, 45],
        'age': [22, 21, 20]}

df = pd.DataFrame(data)

# Writing the DataFrame to a CSV file using a tab separator
df.to_csv('output_file.csv', sep='\t', index=False)

print("DataFrame has been written to 'output_file.csv' using a tab separator.")

"""4.2.15.	Write a Pandas program to replace all the NaN values with Zeros in a column of a data frame. Write a Pandas program to drop a list of rows from a specified data frame."""

import pandas as pd
import numpy as np

# Creating a sample DataFrame with NaN values
data = {'name': ['John', 'Emily', 'Michael', 'Sara', 'David'],
        'score': [85, np.nan, 78, np.nan, 95],
        'age': [20, 21, 19, 22, 20]}

df = pd.DataFrame(data)

# Display original DataFrame
print("Original DataFrame with NaN values:")
print(df)

# Replacing NaN values with zeros in the 'score' column without using inplace
df['score'] = df['score'].fillna(0)

# Display DataFrame after replacing NaN values
print("\nDataFrame after replacing NaN values with zeros in 'score' column:")
print(df)

# Dropping rows with index 1 (Emily) and 3 (Sara)
rows_to_drop = [1, 3]
df = df.drop(rows_to_drop)

# Display DataFrame after dropping the specified rows
print("\nDataFrame after dropping rows with index 1 and 3:")
print(df)

"""4.2.16.	Write a Pandas program to shuffle a given data frame row."""

import pandas as pd

# Creating a sample DataFrame
data = {
    'name': ['John', 'Emily', 'Michael', 'Sara', 'David'],
    'score': [85, 92, 78, 88, 95],
    'age': [20, 21, 19, 22, 20]
}

df = pd.DataFrame(data)

# Display original DataFrame
print("Original DataFrame:")
print(df)

# Shuffling the DataFrame rows
shuffled_df = df.sample(frac=1).reset_index(drop=True)

# Display shuffled DataFrame
print("\nShuffled DataFrame:")
print(shuffled_df)

"""4.2.17.	Write a Pandas program to find the row where the value of a given column is maximum."""

import pandas as pd

# Creating a sample DataFrame
data = {
    'name': ['John', 'Emily', 'Michael', 'Sara', 'David'],
    'score': [85, 92, 78, 88, 95],
    'age': [20, 21, 19, 22, 20]
}

df = pd.DataFrame(data)

# Display original DataFrame
print("Original DataFrame:")
print(df)

# Finding the row where the value of the 'score' column is maximum
max_score_index = df['score'].idxmax()
max_score_row = df.loc[max_score_index]

# Display the row with the maximum score
print("\nRow where the value of 'score' is maximum:")
print(max_score_row)

"""4.2.18.	Write a Pandas program to check whether a given column is present in a data frame or not."""

import pandas as pd

# Creating a sample DataFrame
data = {
    'name': ['John', 'Emily', 'Michael', 'Sara', 'David'],
    'score': [85, 92, 78, 88, 95],
    'age': [20, 21, 19, 22, 20]
}

df = pd.DataFrame(data)

# Display original DataFrame
print("Original DataFrame:")
print(df)

# Column to check
column_to_check = 'score'

# Checking if the column is present in the DataFrame
if column_to_check in df.columns:
    print(f"\nThe column '{column_to_check}' is present in the DataFrame.")
else:
    print(f"\nThe column '{column_to_check}' is not present in the DataFrame.")

"""4.2.19.	Write a Pandas program to append data to an empty data frame."""

import pandas as pd

# Creating an empty DataFrame
df = pd.DataFrame(columns=['name', 'score', 'age'])

# Display the empty DataFrame
print("Empty DataFrame:")
print(df)

# Data to append
data_to_append = [
    {'name': 'John', 'score': 85, 'age': 20},
    {'name': 'Emily', 'score': 92, 'age': 21},
    {'name': 'Michael', 'score': 78, 'age': 19},
    {'name': 'Sara', 'score': 88, 'age': 22},
    {'name': 'David', 'score': 95, 'age': 20}
]

# Creating a DataFrame from the data to append
data_df = pd.DataFrame(data_to_append)

# Appending data to the DataFrame using pd.concat
df = pd.concat([df, data_df], ignore_index=True)

# Display the DataFrame after appending data
print("\nDataFrame after appending data:")
print(df)

"""4.2.20.	Write a Pandas program to convert continuous values of a column in a given data frame to categorical.
Input:
{ 'Name': ['Alberto Franco','Gino Mcneill','Ryan Parkes', 'Eesha Hinton', 'Syed Wharton'], 'Age': [18, 22, 40, 50, 80, 5] }

"""

import pandas as pd

# Creating a sample DataFrame
data = {
    'Name': ['Alberto Franco', 'Gino Mcneill', 'Ryan Parkes', 'Eesha Hinton', 'Syed Wharton'],
    'Age': [18, 22, 40, 50, 80]
}

df = pd.DataFrame(data)

# Display the original DataFrame
print("Original DataFrame:")
print(df)

# Define bins and labels for the age categories
bins = [0, 18, 35, 50, 80]  # Define the edges of the bins
labels = ['Child', 'Young Adult', 'Adult', 'Senior']  # Define the labels for each bin

# Convert the 'Age' column to categorical using pd.cut
df['Age Category'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)

# Display the DataFrame with the new categorical column
print("\nDataFrame with Age Categories:")
print(df)

"""4.2.21.	Write a Pandas program to create data frames that contain random values, missing values, datetime values, and mixed values."""

import pandas as pd
import numpy as np
import random
from datetime import datetime, timedelta

# Set a seed for reproducibility
np.random.seed(42)

# Create a DataFrame with random values
random_values_df = pd.DataFrame({
    'Random Numbers': np.random.randint(1, 100, size=10),
    'Floats': np.random.rand(10),
})

# Create a DataFrame with missing values
missing_values_df = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5],
    'B': [np.nan, 'two', 'three', np.nan, 'five'],
})

# Create a DataFrame with datetime values
dates = [datetime.now() + timedelta(days=i) for i in range(10)]
datetime_values_df = pd.DataFrame({
    'Date': dates,
    'Value': np.random.rand(10)
})

# Create a DataFrame with mixed values
mixed_values_df = pd.DataFrame({
    'Integers': [1, 2, 3, 4, 5],
    'Strings': ['one', 'two', 'three', 'four', 'five'],
    'Booleans': [True, False, True, False, True],
})

# Display the DataFrames
print("DataFrame with Random Values:")
print(random_values_df)

print("\nDataFrame with Missing Values:")
print(missing_values_df)

print("\nDataFrame with Datetime Values:")
print(datetime_values_df)

print("\nDataFrame with Mixed Values:")
print(mixed_values_df)

"""4.2.22.	Write a Pandas program to join the two given data frames along rows and assign all the data."""

import pandas as pd

# Creating the first DataFrame
student_data1 = pd.DataFrame({
    'student_id': ['s1', 's2', 's3', 's4', 's5'],
    'name': ['Danniella Fenton', 'Ryder Storey', 'Bryce Jensen', 'Ed Bernal', 'Kwame Morin'],
    'marks': [200, 210, 190, 222, 199]
})

# Creating the second DataFrame
student_data2 = pd.DataFrame({
    'student_id': ['s4', 's5', 's6', 's7', 's8'],
    'name': ['Scarlette Fisher', 'Carla Williamson', 'Dante Morse', 'Kaiser William', 'Madeeha Preston'],
    'marks': [201, 200, 198, 219, 201]
})

# Displaying the original DataFrames
print("Student Data 1:")
print(student_data1)

print("\nStudent Data 2:")
print(student_data2)

# Joining the two DataFrames along rows
combined_data = pd.concat([student_data1, student_data2], ignore_index=True)

# Displaying the combined DataFrame
print("\nCombined Student Data:")
print(combined_data)

"""4.2.23.	Write a Pandas program to join the two given data frames along columns and assign all the data. (Use the same dataset as above.)"""

import pandas as pd

# Creating the first DataFrame
student_data1 = pd.DataFrame({
    'student_id': ['s1', 's2', 's3', 's4', 's5'],
    'name': ['Danniella Fenton', 'Ryder Storey', 'Bryce Jensen', 'Ed Bernal', 'Kwame Morin'],
    'marks': [200, 210, 190, 222, 199]
})

# Creating the second DataFrame
student_data2 = pd.DataFrame({
    'student_id': ['s4', 's5', 's6', 's7', 's8'],
    'name': ['Scarlette Fisher', 'Carla Williamson', 'Dante Morse', 'Kaiser William', 'Madeeha Preston'],
    'marks': [201, 200, 198, 219, 201]
})

# Displaying the original DataFrames
print("Student Data 1:")
print(student_data1)

print("\nStudent Data 2:")
print(student_data2)

# Joining the two DataFrames along columns
combined_data_columns = pd.concat([student_data1, student_data2], axis=1)

# Displaying the combined DataFrame along columns
print("\nCombined Student Data (along columns):")
print(combined_data_columns)

"""4.2.24.	Write a Pandas program to join the two given data frames along rows and merge with another data frame along the common column id."""

import pandas as pd

# Creating the first DataFrame (student data)
student_data = pd.DataFrame({
    'student_id': ['s1', 's2', 's3', 's4', 's5'],
    'name': ['Danniella Fenton', 'Ryder Storey', 'Bryce Jensen', 'Ed Bernal', 'Kwame Morin'],
    'marks': [200, 210, 190, 222, 199]
})

# Creating the second DataFrame (more student data)
additional_student_data = pd.DataFrame({
    'student_id': ['s4', 's5', 's6', 's7', 's8'],
    'name': ['Scarlette Fisher', 'Carla Williamson', 'Dante Morse', 'Kaiser William', 'Madeeha Preston'],
    'marks': [201, 200, 198, 219, 201]
})

# Joining the two DataFrames along rows
combined_student_data = pd.concat([student_data, additional_student_data], ignore_index=True)

# Creating the exam data DataFrame
exam_data = pd.DataFrame({
    'student_id': ['S1', 'S2', 'S3', 'S4', 'S5', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13'],
    'exam_id': [23, 45, 12, 67, 21, 55, 33, 14, 56, 83, 88, 12]
})

# Displaying the combined student data
print("Combined Student Data:")
print(combined_student_data)

# Merging the combined student data with exam data along the common column 'student_id'
# First, we need to ensure student_id in exam_data matches the format in combined_student_data
exam_data['student_id'] = exam_data['student_id'].str.lower()  # Convert to lowercase

# Merging the DataFrames
merged_data = pd.merge(combined_student_data, exam_data, on='student_id', how='inner')

# Displaying the merged data
print("\nMerged Data (on common column 'student_id'):")
print(merged_data)

"""4.2.25.	Write a Pandas program to join the two data frames with matching records from both sides, where available. (Same dataset as above.)"""

import pandas as pd

# Creating the first DataFrame (student data)
student_data = pd.DataFrame({
    'student_id': ['s1', 's2', 's3', 's4', 's5'],
    'name': ['Danniella Fenton', 'Ryder Storey', 'Bryce Jensen', 'Ed Bernal', 'Kwame Morin'],
    'marks': [200, 210, 190, 222, 199]
})

# Creating the second DataFrame (additional student data)
additional_student_data = pd.DataFrame({
    'student_id': ['s4', 's5', 's6', 's7', 's8'],
    'name': ['Scarlette Fisher', 'Carla Williamson', 'Dante Morse', 'Kaiser William', 'Madeeha Preston'],
    'marks': [201, 200, 198, 219, 201]
})

# Joining the two DataFrames along rows
combined_student_data = pd.concat([student_data, additional_student_data], ignore_index=True)

# Creating the exam data DataFrame
exam_data = pd.DataFrame({
    'student_id': ['s1', 's2', 's3', 's4', 's5', 's7', 's8', 's9', 's10', 's11', 's12', 's13'],
    'exam_id': [23, 45, 12, 67, 21, 55, 33, 14, 56, 83, 88, 12]
})

# Displaying the combined student data
print("Combined Student Data:")
print(combined_student_data)

# Merging the combined student data with exam data on common column 'student_id'
# Use an inner join to keep matching records from both sides
merged_data = pd.merge(combined_student_data, exam_data, on='student_id', how='inner')

# Displaying the merged data with matching records
print("\nMerged Data (matching records from both sides):")
print(merged_data)

"""4.3.	Pandas Grouping
4.3.1.	Write a Pandas program to split the following data frame into groups based on school code. Also, check the type of GroupBy object.

"""

import pandas as pd

# Create the DataFrame
data = {
    'school': ['s001', 's002', 's003', 's001', 's002', 's004'],
    'class': ['V', 'V', 'VI', 'VI', 'V', 'VI'],
    'name': ['Alberto Franco', 'Gino Mcneill', 'Ryan Parkes',
             'Eesha Hinton', 'Gino Mcneill', 'David Parkes'],
    'date_Of_Birth': ['15/05/2002', '17/05/2002', '16/02/1999',
                      '25/09/1998', '11/05/2002', '15/09/1997'],
    'age': [12, 12, 13, 13, 14, 12],
    'height': [173, 192, 186, 167, 151, 159],
    'weight': [35, 32, 33, 30, 31, 32],
    'address': ['street1', 'street2', 'street3', 'street1',
                'street2', 'street4']
}

df = pd.DataFrame(data)

# Group by school code
grouped = df.groupby('school')

# Display the grouped data
for school_code, group in grouped:
    print(f"Group for school code {school_code}:")
    print(group)
    print()

# Check the type of GroupBy object
print(f"Type of GroupBy object: {type(grouped)}")

"""4.3.2.	Write a Pandas program to split the following data frame by school code and get the mean, min, and max values of age for each school. (Use the above dataset.)"""

import pandas as pd

# Create the DataFrame
data = {
    'school': ['s001', 's002', 's003', 's001', 's002', 's004'],
    'class': ['V', 'V', 'VI', 'VI', 'V', 'VI'],
    'name': ['Alberto Franco', 'Gino Mcneill', 'Ryan Parkes',
             'Eesha Hinton', 'Gino Mcneill', 'David Parkes'],
    'date_Of_Birth': ['15/05/2002', '17/05/2002', '16/02/1999',
                      '25/09/1998', '11/05/2002', '15/09/1997'],
    'age': [12, 12, 13, 13, 14, 12],
    'height': [173, 192, 186, 167, 151, 159],
    'weight': [35, 32, 33, 30, 31, 32],
    'address': ['street1', 'street2', 'street3', 'street1',
                'street2', 'street4']
}

df = pd.DataFrame(data)

# Group by school code and calculate mean, min, and max for age
age_stats = df.groupby('school')['age'].agg(['mean', 'min', 'max'])

# Display the result
print(age_stats)

"""4.3.3.	Write a Pandas program to split the following data frame into groups based on all columns and calculate groupby value counts on the data frame.
Test Data:



"""

import pandas as pd

# Create the DataFrame
data = {
    'Id': [1, 2, 1, 1, 2, 1, 2],
    'type': [10, 15, 11, 20, 21, 12, 14],
    'book': ['Math', 'English', 'Physics', 'Math', 'English', 'Physics', 'English']
}

df = pd.DataFrame(data)

# Group by all columns and calculate value counts
grouped_counts = df.groupby(['Id', 'type', 'book']).size().reset_index(name='count')

# Display the result
print(grouped_counts)

"""4.3.4.	Write a Pandas program to split the following data frame into groups by school code and get the mean, min, and max values of age with customized column names for each school."""

import pandas as pd

# Create the DataFrame
data = {
    'school': ['s001', 's002', 's003', 's001', 's002', 's004'],
    'class': ['V', 'V', 'VI', 'VI', 'V', 'VI'],
    'name': ['Alberto Franco', 'Gino Mcneill', 'Ryan Parkes',
             'Eesha Hinton', 'Gino Mcneill', 'David Parkes'],
    'date_Of_Birth': ['15/05/2002', '17/05/2002', '16/02/1999',
                      '25/09/1998', '11/05/2002', '15/09/1997'],
    'age': [12, 12, 13, 13, 14, 12],
    'height': [173, 192, 186, 167, 151, 159],
    'weight': [35, 32, 33, 30, 31, 32],
    'address': ['street1', 'street2', 'street3', 'street1',
                'street2', 'street4']
}

df = pd.DataFrame(data)

# Group by school code and calculate mean, min, and max for age
age_stats = df.groupby('school')['age'].agg(mean_age='mean', min_age='min', max_age='max').reset_index()

# Display the result
print(age_stats)

"""4.4.1.	Visualize the following using the given dataset (alphabet_stock_data.csv),
4.4.1.1.	Create a line plot of the historical stock prices of Alphabet Inc. between two specific dates.

"""

# Step 1: Install libraries (uncomment if needed)
# !pip install pandas matplotlib

# Step 2: Import libraries
import pandas as pd
import matplotlib.pyplot as plt

# Step 3: Load the dataset
df = pd.read_csv('alphabet_stock_data.csv')

# Step 4: Convert the date column to datetime
df['Date'] = pd.to_datetime(df['Date'])

# Step 5: Set the start and end dates for filtering
start_date = '2023-01-01'  # Replace with your start date
end_date = '2023-12-31'    # Replace with your end date

# Filter the DataFrame for the date range
filtered_df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]

# Step 6: Plot the historical stock prices
plt.figure(figsize=(12, 6))
plt.plot(filtered_df['Date'], filtered_df['Close'], label='Closing Price', color='blue')
plt.title('Historical Stock Prices of Alphabet Inc.')
plt.xlabel('Date')
plt.ylabel('Stock Price (USD)')
plt.xticks(rotation=45)
plt.legend()
plt.grid()
plt.tight_layout()
plt.show()

"""4.4.1.2.	Create a bar plot of the trading volume of Alphabet Inc. stock between two specific dates."""

# Step 1: Install libraries (uncomment if needed)
# !pip install pandas matplotlib

# Step 2: Import libraries
import pandas as pd
import matplotlib.pyplot as plt

# Step 3: Load the dataset
df = pd.read_csv('alphabet_stock_data.csv')

# Step 4: Convert the date column to datetime
df['Date'] = pd.to_datetime(df['Date'])

# Step 5: Set the start and end dates for filtering
start_date = '2023-01-01'  # Replace with your start date
end_date = '2023-01-20'    # Replace with your end date

# Filter the DataFrame for the date range
filtered_df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]

# Step 6: Plot the trading volume
plt.figure(figsize=(12, 6))
plt.bar(filtered_df['Date'], filtered_df['Volume'], color='orange')
plt.title('Trading Volume of Alphabet Inc. Stock')
plt.xlabel('Date')
plt.ylabel('Trading Volume')
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

"""4.4.1.3.	Create a stacked histogram plot with more bins of opening, closing, high, and low stock prices of Alphabet Inc. between two specific dates."""

# Step 1: Install libraries (uncomment if needed)
# !pip install pandas matplotlib

# Step 2: Import libraries
import pandas as pd
import matplotlib.pyplot as plt

# Step 3: Load the dataset
df = pd.read_csv('alphabet_stock_data.csv')

# Step 4: Convert the date column to datetime
df['Date'] = pd.to_datetime(df['Date'])

# Step 5: Set the start and end dates for filtering
start_date = '2023-01-01'  # Replace with your start date
end_date = '2023-01-20'    # Replace with your end date

# Filter the DataFrame for the date range
filtered_df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]

# Step 6: Create a stacked histogram
plt.figure(figsize=(12, 6))
plt.hist(filtered_df['Open'], bins=30, alpha=0.5, label='Opening Price', color='blue')
plt.hist(filtered_df['Close'], bins=30, alpha=0.5, label='Closing Price', color='green')
plt.hist(filtered_df['High'], bins=30, alpha=0.5, label='High Price', color='orange')
plt.hist(filtered_df['Low'], bins=30, alpha=0.5, label='Low Price', color='red')

plt.title('Stacked Histogram of Alphabet Inc. Stock Prices')
plt.xlabel('Stock Price (USD)')
plt.ylabel('Frequency')
plt.legend(loc='upper right')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

"""4.4.1.4.	Create a scatter plot of the trading volume/stock prices of Alphabet Inc. stock between two specific dates."""

# Step 1: Install libraries (uncomment if needed)
# !pip install pandas matplotlib

# Step 2: Import libraries
import pandas as pd
import matplotlib.pyplot as plt

# Step 3: Load the dataset
df = pd.read_csv('alphabet_stock_data.csv')

# Step 4: Convert the date column to datetime
df['Date'] = pd.to_datetime(df['Date'])

# Step 5: Set the start and end dates for filtering
start_date = '2023-01-01'  # Replace with your start date
end_date = '2023-01-20'    # Replace with your end date

# Filter the DataFrame for the date range
filtered_df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]

# Step 6: Create a scatter plot
plt.figure(figsize=(12, 6))
plt.scatter(filtered_df['Close'], filtered_df['Volume'], color='purple', alpha=0.5)
plt.title('Scatter Plot of Trading Volume vs. Closing Prices of Alphabet Inc.')
plt.xlabel('Closing Price (USD)')
plt.ylabel('Trading Volume')
plt.grid()
plt.tight_layout()
plt.show()

"""4.4.2.	Write a Python program to draw a line with a suitable label on the x-axis, y-axis, and title."""

import matplotlib.pyplot as plt

# Sample data for the line plot
x = [1, 2, 3, 4, 5]  # X-axis data
y = [2, 3, 5, 7, 11]  # Y-axis data

# Create the line plot
plt.figure(figsize=(10, 5))
plt.plot(x, y, marker='o', linestyle='-', color='b')  # You can customize marker and linestyle

# Adding labels and title
plt.title('Sample Line Plot')
plt.xlabel('X-axis Label')  # Replace with your x-axis label
plt.ylabel('Y-axis Label')  # Replace with your y-axis label

# Show grid
plt.grid()

# Show the plot
plt.tight_layout()
plt.show()

"""4.4.3.	Write a Python program to draw line charts of the financial data of Alphabet Inc. between October 3, 2016, and October 7, 2016.
Date,Open,High,Low,Close
10-03-16,774.25,776.065002,769.5,772.559998
10-04-16,776.030029,778.710022,772.890015,776.429993
10-05-16,779.309998,782.070007,775.650024,776.469971
10-06-16,779,780.47998,775.539978,776.859985
10-07-16,779.659973,779.659973,770.75,775.080017

"""

import pandas as pd
import matplotlib.pyplot as plt

# Sample financial data for Alphabet Inc.
data = {
    'Date': ['10-03-16', '10-04-16', '10-05-16', '10-06-16', '10-07-16'],
    'Open': [774.25, 776.03, 779.31, 779, 779.66],
    'High': [776.065002, 778.710022, 782.070007, 780.47998, 779.659973],
    'Low': [769.5, 772.890015, 775.650024, 775.539978, 770.75],
    'Close': [772.559998, 776.429993, 776.469971, 776.859985, 775.080017]
}

# Create a DataFrame
df = pd.DataFrame(data)

# Convert the 'Date' column to datetime format
df['Date'] = pd.to_datetime(df['Date'], format='%m-%d-%y')

# Set the 'Date' column as the index
df.set_index('Date', inplace=True)

# Plotting the financial data
plt.figure(figsize=(12, 6))

# Line charts for Open, High, Low, and Close prices
plt.plot(df.index, df['Open'], marker='o', label='Open', linestyle='-')
plt.plot(df.index, df['High'], marker='o', label='High', linestyle='-')
plt.plot(df.index, df['Low'], marker='o', label='Low', linestyle='-')
plt.plot(df.index, df['Close'], marker='o', label='Close', linestyle='-')

# Adding title and labels
plt.title('Financial Data of Alphabet Inc. (Oct 3, 2016 - Oct 7, 2016)')
plt.xlabel('Date')
plt.ylabel('Stock Price (USD)')
plt.xticks(rotation=45)
plt.grid()
plt.legend()  # Show legend
plt.tight_layout()

# Show the plot
plt.show()

"""4.4.4.	Write a Python program to draw a line with a suitable label on the x-axis, and y-axis and a title.
Create the code snippet that gives the output shown in the following screenshot:

"""

import matplotlib.pyplot as plt

# Data for plotting
x = [10, 15, 20, 25, 30]
y1 = [10, 20, 30, 20, 10]  # Blue line data (touches lower boundary at 10)
y2 = [40, 30, 20, 30, 40]  # Red line data (touches upper boundary at 40)

# Create the plot with specific line styles
plt.plot(x, y1, 'b:', label='line1-dotted', linewidth=1)  # Blue dotted line, thinner
plt.plot(x, y2, 'r--', label='line2-dashed', linewidth=5)  # Red dashed line, thicker

# Add labels and title
plt.xlabel('x - axis')
plt.ylabel('y - axis')
plt.title('Plot with two or more lines with different styles')

# Set x and y-axis limits to ensure the lines touch the boundaries
plt.xlim(10, 30)
plt.ylim(10, 40)

# Add a legend
plt.legend()

# Show the plot
plt.show()

"""4.4.5.	Write a Python program to display the grid and draw line charts of the closing value of Alphabet Inc. between October 3, 2016, and October 7, 2016. Customized the grid lines with linestyle -, width 0.5, and color blue.
Date,Close
03-10-16,772.559998
04-10-16,776.429993
05-10-16,776.469971
06-10-16,776.859985
07-10-16,775.080017

"""

import matplotlib.pyplot as plt
import pandas as pd

# Data: Date and Closing value of Alphabet Inc.
dates = ['03-10-16', '04-10-16', '05-10-16', '06-10-16', '07-10-16']
close_values = [772.559998, 776.429993, 776.469971, 776.859985, 775.080017]

# Convert date strings into a pandas datetime format
dates = pd.to_datetime(dates, format='%d-%m-%y')

# Plotting the data
plt.plot(dates, close_values, marker='o', linestyle='-', color='green')

# Adding title and labels
plt.title('Alphabet Inc. Closing Value (Oct 3, 2016 - Oct 7, 2016)')
plt.xlabel('Date')
plt.ylabel('Closing Value')

# Customizing the grid with linestyle, width, and color
plt.grid(True, linestyle='-', linewidth=0.5, color='blue')

# Display the plot
plt.show()

"""4.4.6.	Write a Python program to create multiple plots as in the screenshot (use any method)."""

import matplotlib.pyplot as plt

# Create figure
fig = plt.figure(figsize=(8, 6))

# Add the large plot (1 row, 1 column)
ax1 = plt.subplot2grid((2, 3), (0, 0), colspan=3)

# Add the three smaller plots below (3 columns)
ax2 = plt.subplot2grid((2, 3), (1, 0))
ax3 = plt.subplot2grid((2, 3), (1, 1))
ax4 = plt.subplot2grid((2, 3), (1, 2))

# Plot dummy data (optional)
ax1.plot([0, 1], [0, 1])
ax1.set_title('Large Plot')

ax2.plot([0, 1], [0, 1])
ax2.set_title('Small Plot 1')

ax3.plot([0, 1], [0, 1])
ax3.set_title('Small Plot 2')

ax4.plot([0, 1], [0, 1])
ax4.set_title('Small Plot 3')

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()

"""4.4.7.	Write a Python program to create a bar plot from a data frame.
Sample Data Frame: s
a b c d e f
2 4,8,5,7,6
4 2,3,4,2,6
6 4,7,4,7,8
8 2,6,4,8,6
10 2,4,3,3,2
Create the code snippet which gives the output shown in the following screenshot:

"""

import pandas as pd
import matplotlib.pyplot as plt

# Sample data
data = {
    'a': [4, 2, 4, 2, 2],
    'b': [8, 3, 7, 6, 4],
    'c': [5, 4, 4, 4, 3],
    'd': [7, 2, 7, 8, 3],
    'e': [6, 6, 8, 6, 2]
}

# Create DataFrame
index = [2, 4, 6, 8, 10]
df = pd.DataFrame(data, index=index)

# Plotting the bar plot
df.plot(kind='bar', width=0.8)

# Adding legend, labels, and grid
plt.legend(title="s", loc='best')
plt.grid(True)
plt.show()

"""4.4.8.	Write a Python program to create a stacked bar plot with error bars.
Note: Use the bottom to stack the women's bars on top of the men's bars.
Sample Data:
Means (men) = (22, 30, 35, 35, 26)
Means (women) = (25, 32, 30, 35, 29)
Men's Standard deviation = (4, 3, 4, 1, 5)
Women's Standard deviation = (3, 5, 2, 3, 3)
Create the code snippet that gives the output shown in the following
"""

import numpy as np
import matplotlib.pyplot as plt

# Data
means_men = (22, 30, 35, 35, 26)
means_women = (25, 32, 30, 35, 29)

std_men = (4, 3, 4, 1, 5)
std_women = (3, 5, 2, 3, 3)

# Group labels
groups = ('Group1', 'Group2', 'Group3', 'Group4', 'Group5')

# X locations for the groups
x = np.arange(len(groups))

# Plotting the men's bars
plt.bar(x, means_men, yerr=std_men, color='r', width=0.6, label='Men')

# Plotting the women's bars on top of the men's bars (stacked)
plt.bar(x, means_women, yerr=std_women, color='g', width=0.6, bottom=means_men, label='Women')

# Adding labels and title
plt.ylabel('Scores')
plt.title('Scores by group\nand gender')

# Adding group labels to the x-axis
plt.xticks(x, groups)

# Adding a legend
plt.legend()

# Display the plot
plt.show()

"""4.4.9.	Write a Python program to create stack bar plot and add labels to each section.
Sample data:
people = ('G1','G2','G3','G4','G5','G6','G7','G8')
segments = 4
# multi-dimensional data
data = [[ 3.40022085, 7.70632498, 6.4097905, 10.51648577, 7.5330039, 7.1123587, 12.77792868, 3.44773477],
[ 11.24811149, 5.03778215, 6.65808464, 12.32220677, 7.45964195, 6.79685302, 7.24578743, 3.69371847],
[ 3.94253354, 4.74763549, 11.73529246, 4.6465543, 12.9952182, 4.63832778, 11.16849999, 8.56883433],
[ 4.24409799, 12.71746612, 11.3772169, 9.00514257, 10.47084185, 10.97567589, 3.98287652, 8.80552122]]
Create the code snippet that gives the output shown in the following screenshot:

"""

import numpy as np
import matplotlib.pyplot as plt

# Sample data
people = ('G1','G2','G3','G4','G5','G6','G7','G8')
segments = 4
data = np.array([
    [ 3.40022085, 7.70632498, 6.4097905, 10.51648577, 7.5330039, 7.1123587, 12.77792868, 3.44773477],
    [ 11.24811149, 5.03778215, 6.65808464, 12.32220677, 7.45964195, 6.79685302, 7.24578743, 3.69371847],
    [ 3.94253354, 4.74763549, 11.73529246, 4.6465543, 12.9952182, 4.63832778, 11.16849999, 8.56883433],
    [ 4.24409799, 12.71746612, 11.3772169, 9.00514257, 10.47084185, 10.97567589, 3.98287652, 8.80552122]
])

# Assign colors for each segment
colors = ['red', 'green', 'white', 'purple']

# Set up the bar positions
index = np.arange(len(people))

# Initialize the bottom positions for stacking
bottom = np.zeros(len(people))

# Create the plot
fig, ax = plt.subplots()

# Plot each segment
for i in range(segments):
    ax.barh(index, data[i], left=bottom, color=colors[i % len(colors)], label=f'Segment {i+1}')
    # Add text labels on each segment
    for j in range(len(people)):
        ax.text(bottom[j] + data[i][j]/2, j, f'{int(data[i][j])}%', ha='center', va='center', color='black')
    bottom += data[i]

# Add labels and title
ax.set(yticks=index, yticklabels=people, xlabel='Scores')
ax.set_title('Stacked Bar Plot with Labels')

# Display the legend
ax.legend()

# Show the plot
plt.show()

"""4.4.10.	Write a Python program to add textures (black and white) to bars and wedges.
Note: Use the bottom to stack the women's bars on top of the men's bars. Create the code snippet that gives the output shown in the following screenshot:

"""

import matplotlib.pyplot as plt
import numpy as np

# Sample data
men_means = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
women_means = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

# Define x-axis labels
x = np.arange(len(men_means))

# Define hatch patterns (textures)
hatch_patterns = ['/', '\\', '|', '-', '+', 'x', 'o', 'O', '.', '*']

fig, ax = plt.subplots()

# Plot men's bars with black edge and white face
bars_men = ax.bar(x, men_means, color='white', edgecolor='black', hatch=hatch_patterns)

# Plot women's bars on top of men's bars using the bottom parameter, also black and white
bars_women = ax.bar(x, women_means, bottom=men_means, color='white', edgecolor='black', hatch=hatch_patterns)

# Set labels and title
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Bar Plot with Black and White Textures')

# Display the plot
plt.show()

"""5.	Data Analytics using Python
5.1.	Handle the given dataset (Data.csv) with adequate preprocessing steps mentioned and visualize the dataset with appropriate graphs.
5.1.1.	Handle Missing Data Values
5.1.2.	Encode the categorical data
5.1.3.	Scale your features

"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load the dataset from the local CSV file
data = pd.read_csv('Data.csv')  # Ensure the Data.csv file is in the same directory as the script

print("Original Dataset:\n", data)

# Step 5.1.1: Handle Missing Data Values
# Filling missing Age and Salary with the mean of the respective columns
data['Age'] = data['Age'].fillna(data['Age'].mean())  # Assign the result back to the 'Age' column
data['Salary'] = data['Salary'].fillna(data['Salary'].mean())  # Assign the result back to the 'Salary' column

print("\nDataset after handling missing values:\n", data)

# Step 5.1.2: Encode the categorical data
# Encoding 'Country' and 'Purchased' columns
label_encoder = LabelEncoder()

# Encode Purchased (Yes/No) into 1/0
data['Purchased'] = label_encoder.fit_transform(data['Purchased'])

# Encode 'Country' using OneHotEncoding (creates separate columns for each category)
data = pd.get_dummies(data, columns=['Country'], drop_first=True)  # drop_first to avoid dummy variable trap

print("\nDataset after encoding categorical variables:\n", data)

# Step 5.1.3: Scale your features
scaler = StandardScaler()
data[['Age', 'Salary']] = scaler.fit_transform(data[['Age', 'Salary']])

print("\nDataset after scaling features:\n", data)

# Step 5.1.4: Visualization
# Visualization of the dataset using various plots

# Plot 1: Age distribution
plt.figure(figsize=(8, 4))
sns.histplot(data['Age'], kde=True)
plt.title('Age Distribution')
plt.show()

# Plot 2: Salary distribution
plt.figure(figsize=(8, 4))
sns.histplot(data['Salary'], kde=True, color='red')
plt.title('Salary Distribution')
plt.show()

# Plot 3: Count of Purchases (Purchased or Not)
plt.figure(figsize=(6, 4))
sns.countplot(x='Purchased', data=data)
plt.title('Count of Purchases (Yes/No)')
plt.show()

# Plot 4: Country-wise Purchase
plt.figure(figsize=(8, 4))
sns.barplot(x='Purchased', y='Salary', hue='Country_France', data=data)
plt.title('Country-wise Salary of Purchasers')
plt.show()

"""5.2.	Using the given dataset (dirtydata.csv),
5.2.1.	Handle the data with empty cells (Use dropna() and fillna())
5.2.2.	Replace the empty cells using mean, median, and mode.
5.2.3.	Handle the data in the wrong format.
5.2.4.	Handle the wrong data from the dataset.
5.2.5.	Discover and remove duplicates.

"""

import pandas as pd
import numpy as np

# Step 1: Load the dataset
data = pd.read_csv('dirtydata.csv')

print("Original Dataset:\n", data)

# Step 5.2.1: Handle the data with empty cells
# Using dropna() - Drop rows with any missing values
data_dropped = data.dropna()
print("\nDataset after dropping rows with missing values:\n", data_dropped)

# Using fillna() - Fill missing values with a placeholder (e.g., 'Unknown')
data_filled = data.fillna("Unknown")
print("\nDataset after filling missing values with 'Unknown':\n", data_filled)

# Step 5.2.2: Replace empty cells using mean, median, and mode
# Filling missing 'Age' with mean, 'Salary' with median
data['Age'] = data['Age'].fillna(data['Age'].mean())
data['Salary'] = pd.to_numeric(data['Salary'], errors='coerce')  # Convert invalid salary to NaN
data['Salary'] = data['Salary'].fillna(data['Salary'].median())

print("\nDataset after filling missing values using mean for Age and median for Salary:\n", data)

# Step 5.2.3: Handle the data in the wrong format
# Correcting the date format by replacing 'wrong_date' with a valid date or marking it as NaT (Not a Time)
data['Date_of_Joining'] = pd.to_datetime(data['Date_of_Joining'], errors='coerce')

print("\nDataset after handling wrong date format:\n", data)

# Step 5.2.4: Handle wrong data in the dataset
# For example, invalid 'Salary' values have been set to NaN in step 5.2.2 using pd.to_numeric()
# You can also apply conditions to identify and correct wrong data

# Step 5.2.5: Discover and remove duplicates
# Checking for duplicates based on all columns
duplicates = data.duplicated()
print("\nDuplicate entries:\n", data[duplicates])

# Removing duplicates
data_cleaned = data.drop_duplicates()
print("\nDataset after removing duplicates:\n", data_cleaned)

"""5.3.	Create a cricketer dataset using a dictionary of lists, and create a new attribute ‘Experience Category’ using ‘Age’ as the binning factor."""

import pandas as pd

# Step 1: Create the cricketer dataset using a dictionary of lists
data = {
    'Name': ['Virat Kohli', 'Rohit Sharma', 'MS Dhoni', 'Sachin Tendulkar', 'Hardik Pandya', 'Ravindra Jadeja', 'Shubman Gill', 'Rishabh Pant', 'Yuvraj Singh'],
    'Age': [34, 36, 39, 47, 28, 32, 24, 26, 41],
    'Runs': [12000, 10000, 10500, 18426, 3500, 2400, 1800, 2500, 8700],
    'Wickets': [4, 8, 1, 154, 55, 185, 0, 0, 111],
    'Matches': [250, 200, 350, 463, 80, 100, 30, 50, 300]
}

# Convert the dictionary to a pandas DataFrame
cricketer_df = pd.DataFrame(data)

print("Original Cricketer Dataset:\n", cricketer_df)

# Step 2: Create 'Experience Category' using 'Age' as the binning factor
# Define the bins and labels for the experience category
bins = [0, 25, 35, 50]
labels = ['Young', 'Mid-level', 'Experienced']

# Create the new column 'Experience Category' by binning the 'Age'
cricketer_df['Experience Category'] = pd.cut(cricketer_df['Age'], bins=bins, labels=labels, right=False)

print("\nCricketer Dataset with 'Experience Category':\n", cricketer_df)

"""5.4.	car_age = [5, 7, 8, 7, 2, 17, 2, 9, 4, 11, 12, 9, 6]
car_speed = [99,86,87,88,111,86,103,87,94,78,77,85,86]
Using the given dataset,
5.4.1.	Draw the line of linear regression
5.4.2.	Evaluate how well the data fit in linear regression.
5.4.3.	Predict the speed of a 10-year-old car

"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Data
car_age = np.array([5, 7, 8, 7, 2, 17, 2, 9, 4, 11, 12, 9, 6])
car_speed = np.array([99, 86, 87, 88, 111, 86, 103, 87, 94, 78, 77, 85, 86])

# Reshape data for sklearn
X = car_age.reshape(-1, 1)
y = car_speed

# Fit the model
model = LinearRegression().fit(X, y)
y_pred = model.predict(X)

# Linear regression parameters
m = model.coef_[0]
c = model.intercept_

print(f'Regression Line: y = {m:.2f}x + {c:.2f}')

# Plot data points
plt.scatter(car_age, car_speed, color='blue', label='Data points')

# Plot regression line
x_values = np.linspace(min(car_age), max(car_age), 100)
y_values = m * x_values + c
plt.plot(x_values, y_values, color='red', label='Regression line')

plt.xlabel('Car Age')
plt.ylabel('Car Speed')
plt.title('Car Age vs Speed with Linear Regression Line')
plt.legend()
plt.show()

# Calculate R^2 score
r2 = r2_score(y, y_pred)
print(f'R^2 score: {r2:.2f}')

# Predict speed for a 10-year-old car
age_to_predict = 10
predicted_speed = model.predict([[age_to_predict]])[0]
print(f'Predicted speed for a {age_to_predict}-year-old car: {predicted_speed:.2f} mph')

"""5.5.	Using the dataset (cars.csv),
5.5.1.	Predict the CO2 emissions of a car with a weight of 2300 kg and volume of 1300 cm3.
5.5.2.	Print the coefficient values of the regression object.

"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Load dataset
df = pd.read_csv('cars.csv')

# Display the first few rows of the dataframe to understand its structure
print(df.head())

# Extract features and target variable
X = df[['Weight', 'Volume']]
y = df['CO2_Emissions']

# Create and fit the model
model = LinearRegression()
model.fit(X, y)

# Print the coefficients of the regression model
print(f'Coefficients: {model.coef_}')
print(f'Intercept: {model.intercept_}')

# Create a DataFrame for the prediction
prediction_data = pd.DataFrame({'Weight': [2300], 'Volume': [1300]})

# Predict the CO2 emissions for the car with the given weight and volume
predicted_emissions = model.predict(prediction_data)

print(f'Predicted CO2 emissions for a car with weight 2300 kg and volume 1300 cm3: {predicted_emissions[0]:.2f} g/km')

"""5.6.	Using the insurance dataset (insurance.csv) with adequate preprocessing steps,
5.6.1.	Visualize the correlation among variables using a heatmap.

"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('insurance.csv')

# Show basic information about the dataset
print(df.info())

# Display the first few rows of the dataset
print(df.head())

# Preprocess the data
# Convert categorical columns to numerical if needed (e.g., using one-hot encoding)
df_encoded = pd.get_dummies(df)

# Calculate the correlation matrix
corr_matrix = df_encoded.corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

"""5.6.2.	Create a linear regression model."""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('insurance.csv')

# Show basic information about the dataset
print(df.info())
print(df.head())

# Preprocess the data
# Convert categorical columns to numerical using one-hot encoding
df_encoded = pd.get_dummies(df)

# Define features and target
# Assume 'charges' is the target variable and the rest are features
X = df_encoded.drop('charges', axis=1)  # Features
y = df_encoded['charges']  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error (MSE): {mse}')
print(f'Mean Absolute Error (MAE): {mae}')
print(f'R-squared (R2): {r2}')

# Optional: Plotting actual vs. predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel('Actual Charges')
plt.ylabel('Predicted Charges')
plt.title('Actual vs. Predicted Charges')
plt.show()

"""5.6.3.	Evaluate the model.  (Find MSE and R_square.)"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
df = pd.read_csv('insurance.csv')

# Preprocess the data
# Convert categorical columns to numerical using one-hot encoding
df_encoded = pd.get_dummies(df)

# Define features and target
X = df_encoded.drop('charges', axis=1)  # Features
y = df_encoded['charges']  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error (MSE): {mse}')
print(f'R-squared (R2): {r2}')

"""5.6.4.	Predict the charges for a person with an age of 30, a BMI of 32.00, and who is a smoker."""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
df = pd.read_csv('insurance.csv')

# Preprocess the data
# Convert categorical columns to numerical using one-hot encoding
df_encoded = pd.get_dummies(df)

# Define features and target
X = df_encoded.drop('charges', axis=1)  # Features
y = df_encoded['charges']  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Retrieve feature names
feature_names = X_train.columns

# Define the input for the new person
# Create a DataFrame with all features, even if they are zero
new_person = pd.DataFrame([{
    'age': 30,
    'bmi': 32.00,
    'sex_female': 0,  # Example, adjust based on encoding
    'sex_male': 1,    # Example, adjust based on encoding
    'smoker_no': 0,   # Example, adjust based on encoding
    'smoker_yes': 1,  # Example, adjust based on encoding
    'region_northeast': 0,  # Example, adjust based on encoding
    'region_northwest': 0,  # Example, adjust based on encoding
    'region_southeast': 0,  # Example, adjust based on encoding
    'region_southwest': 0,  # Example, adjust based on encoding
    'children': 0         # Add missing feature if necessary
}], columns=feature_names)

# Predict the charges for the new person
predicted_charges = model.predict(new_person)

print(f'Predicted charges: ${predicted_charges[0]:,.2f}')

"""5.7.	Evaluate the dataset (User_Data.csv) and predict whether a user will purchase the company's product or not. (Use logistic regression.)"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('User_Data.csv')

# Inspect the dataset
print(df.info())
print(df.head())

# Preprocess the data
# Handle missing values (if any)
df = df.dropna()  # Dropping missing values; you may choose to impute instead

# Convert categorical columns to numerical using Label Encoding or One-Hot Encoding
label_encoders = {}
for column in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le

# Define features and target
X = df.drop('Purchased', axis=1)  # Replace 'Purchased' with the actual name of the target column
y = df['Purchased']  # Replace 'Purchased' with the actual name of the target column

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features (optional but recommended for better performance)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize and train the Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Purchased', 'Purchased'],
            yticklabels=['Not Purchased', 'Purchased'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names
target_names = iris.target_names

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train the Decision Tree Classifier
clf = DecisionTreeClassifier(max_depth=4, random_state=42)
clf.fit(X_train, y_train)

# Plot the decision tree
plt.figure(figsize=(20,10))
plot_tree(clf, feature_names=feature_names, class_names=target_names, filled=True)
plt.title('Decision Tree with Depth=4')
plt.savefig('decision_tree.png')  # Save the plot as a PNG file
plt.show()

# Make predictions
y_pred = clf.predict(X_test)

# Print the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Print the classification report
class_report = classification_report(y_test, y_pred, target_names=target_names)
print("Classification Report:")
print(class_report)

# Optionally, plot the confusion matrix
plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""5.9.	Use the KNN algorithm to train the model and predict the future using the Iris dataset. Also, measure the accuracy of the model."""

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names
target_names = iris.target_names

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the KNN Classifier
# You can experiment with different values of k
k = 5  # Choose the number of neighbors
knn = KNeighborsClassifier(n_neighbors=k)

# Train the model
knn.fit(X_train, y_train)

# Make predictions
y_pred = knn.predict(X_test)

# Measure the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy of KNN model with k={k}: {accuracy:.2f}')

# Print the classification report
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=target_names))

# Print the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Plot the confusion matrix
plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for KNN')
plt.show()

"""5.10.	Analyze the given dataset (gym_data.csv) using RandomForestRegressor and visualize the ‘Effect of n_estimators."""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Load the dataset
df = pd.read_csv('gym_data.csv')

# Inspect the dataset
print(df.head())
print(df.info())

# Assume that the dataset has columns 'feature1', 'feature2', ..., 'featureN', and 'target'
# You need to replace 'feature1', ..., 'featureN', and 'target' with the actual column names
X = df.drop('target', axis=1)  # Replace 'target' with the actual target column name
y = df['target']  # Replace 'target' with the actual target column name

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define different values for n_estimators
n_estimators_list = [10, 50, 100, 200, 300]
mse_list = []

# Train and evaluate RandomForestRegressor with different n_estimators
for n in n_estimators_list:
    rf = RandomForestRegressor(n_estimators=n, random_state=42)
    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mse_list.append(mse)
    print(f'n_estimators: {n}, MSE: {mse:.2f}')

# Plot the effect of n_estimators on MSE
plt.figure(figsize=(10, 6))
plt.plot(n_estimators_list, mse_list, marker='o')
plt.xlabel('Number of Estimators')
plt.ylabel('Mean Squared Error')
plt.title('Effect of n_estimators on RandomForestRegressor Performance')
plt.grid(True)
plt.show()

"""5.11.	Visualize a 3-dimensional cluster using the given dataset ‘Mall_Customers.csv’, where no_of_clusters = 5."""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Load the dataset
df = pd.read_csv('Mall_Customers.csv')

# Display the first few rows of the dataset
print(df.head())

# Select relevant features for clustering
# Ensure these columns exist in your dataset; adjust names if needed
X = df[['Annual Income (k$)', 'Spending Score (1-100)', 'Age']]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means clustering
n_clusters = 5
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
df['Cluster'] = kmeans.fit_predict(X_scaled)

# Reduce dimensions to 3D using PCA for visualization
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_scaled)

# Create a 3D scatter plot
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# Plot the data points
scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=df['Cluster'], cmap='viridis', marker='o')

# Add color bar
legend1 = ax.legend(*scatter.legend_elements(), title="Clusters")
ax.add_artist(legend1)

# Label the axes and add a title
ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_zlabel('Principal Component 3')
ax.set_title('3D Clustering Visualization (K-Means with n_clusters=5)')

plt.show()

"""5.12.	Using the dataset provided (Online Retail.xlsx),
5.12.1.	Split the data according to the region of the transaction.
5.12.2.	Build the models using the apriori algorithm.
5.12.3.	Develop the association rules.
5.12.4.	Find the most frequent items in any one of the regions.

5.12.2 Build the Models Using the Apriori Algorithm
"""

import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

# Load the CSV dataset
file_path = 'Online Retail.csv'
df = pd.read_csv(file_path)

# Display the first few rows of the dataset
print(df.head())

# Split the data by region (assuming 'Country' is the region column)
regions = df['Country'].unique()
region_dfs = {region: df[df['Country'] == region] for region in regions}

# Example: data for one region, e.g., 'United Kingdom'
uk_data = region_dfs['United Kingdom']

# Prepare the data for Apriori
def prepare_data(df):
    df = df.dropna(subset=['InvoiceNo', 'Description'])
    basket = df.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')
    basket = basket.apply(lambda x: x > 0)  # Convert to boolean
    return basket

basket_uk = prepare_data(uk_data)

# Apply the Apriori algorithm
frequent_itemsets = apriori(basket_uk, min_support=0.01, use_colnames=True)
print('Frequent Itemsets:')
print(frequent_itemsets.head())

# Generate association rules
rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.5)
print('Association Rules:')
print(rules.head())

# Find the most frequent items
most_frequent_items = frequent_itemsets.sort_values(by='support', ascending=False)
print('Most Frequent Items:')
print(most_frequent_items.head())